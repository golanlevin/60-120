# Tuesday, January 21

### Agenda

* [Effet Kazoo](https://www.youtube.com/watch?v=y9FKxMiiI6Y)
* Attendance
* Review of [1.3](https://github.com/golanlevin/60-120/tree/main/2025/assignments/generative_ai#13-exercise-text-to-image-synthesis-with-midjourney), [1.4](https://github.com/golanlevin/60-120/tree/main/2025/assignments/generative_ai#14-exercise-image-outpainting-with-runwayml)
* Lecture on Dataset Bias
* Work Session
* Reminder about 1/23

---

## Lecture(s) on Generative AI

* [Seeing Like a Computer](https://golancourses.net/60120/daily-notes/unit-1/machine-learning-art-ii/)
* [The Dark Side of AI and the Arts](https://golancourses.net/60120/daily-notes/unit-1/the-dark-side-of-ai-and-the-arts/)

---

## Lecture on Dataset Bias

*(Lecture materials by Paolo Pedercini)*

[Excavating AI by Kate Crawford and Trevor Paglen](https://www.youtube.com/watch?v=P4JpD1PWBDI)

![excavating.png](img/excavating.png)

[Deep Hysteria by Amy Alexander](https://amy-alexander.com/projects/deep-hysteria/)

One AI generates "twins" that vary only in gender presentation. Another "AI", trained on human perceptions, attempts to identify the emotion on their faces. The female-presenting faces are often categorized as upset, confused, etc. 

![alexander_amy.png](img/alexander_amy.png)

Various types of racial and gender bias have been affecting AI systems since their inception. For example, in [AI-generated barbies](https://www.youtube.com/watch?v=L2sQRrf1Cd8&t=105s)

[![barbie.png](img/barbie.png)](https://www.youtube.com/watch?v=L2sQRrf1Cd8&t=105s)

Also see the [Stable Diffusion Bias Explorer](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer)

![stable-explorer.png](img/stable-explorer.png)

DALL-E3 attempts to combat the racial bias in its training data by occasionally randomly inserting race words that aren’t white into a prompt. These can bubble up accidentally: 

> Prompt: *Guy with swords pointed at me meme except they are Homer Simpson.*

![homer-simpson-ai.png](img/homer-simpson-ai.png)

Object recognition can be affected by skin color.

![googlevision.png](img/googlevision.png)


["The viral AI avatar app Lensa undressed me—without my consent"](https://www.technologyreview.com/2022/12/12/1064751/the-viral-ai-avatar-app-lensa-undressed-me-without-my-consent/)
> *My avatars were cartoonishly pornified, while my male colleagues got to be astronauts, explorers, and inventors*.

![heikkila.png](img/heikkila.png)

### Additional References: 

*Beyond media representation, why does this matter?*

* [Flawed facial recognition system sent a man to jail](https://www.wired.com/story/flawed-facial-recognition-system-sent-man-jail/?ref=dl-staging-website.ghost.io)
* [Predictive policing](https://www.technologyreview.com/2021/02/05/1017560/predictive-policing-racist-algorithmic-bias-data-crime-predpol/)
* [AI used to deny health insurance](https://www.cbsnews.com/news/unitedhealth-lawsuit-ai-deny-claims-medicare-advantage-health-insurance-denials/) and to [appeal such denials](https://qz.com/fight-health-insurance-denials-appeals-ai-1851733712)
* [AI tool used to predict the need of extra medical care is biased against Black patients](https://www.technologyreview.com/2019/10/25/132184/a-biased-medical-algorithm-favored-white-people-for-healthcare-programs/)
* [AI-powered recruiting tool is biased against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G/) (luckily scrapped by Amazon)
* [AI tools are used to grade student essays](https://www.vice.com/en/article/flawed-algorithms-are-grading-millions-of-students-essays/)
* [AI tools are used to review college admissions](https://www.thenation.com/article/society/artificial-intelligence-chatgpt-college-applications/)